# Table of Contents
* [L02 - Algorithms](#l02---algorithms)
	* [Example: Fibonacci Numbers](#example-fibonacci-numbers)
	* [Memoization](#memoization)
	* [Complexity Analysis: General Method](#complexity-analysis-general-method)
* [L03 - Complexity Analysis](#l03---complexity-analysis)
	* [Big-O Definition](#big-o-definition)
	* [Big-O Arithmetic](#big-o-arithmetic)
	* [Big-O Hierarchy](#big-o-hierarchy)
	* [Big Omega Ω (Lower Bound) and Big Theta Ө (Growth Rate)](#big-omega-Ω-(lower-bound)-and-big-theta-Ө-(growth-rate))
* [L04 - Data Structures 101](#l04---data-structures-101)
	* [Abstract Data Types vs. Data Structures](#abstract-data-types-vs.-data-structures)
	* [Data Structures and Searching](#data-structures-and-searching)
	* [malloc()](#malloc())
* [L05 - Data Structures, The Basics](#l05---data-structures,-the-basics)
	* [Linked Lists](#linked-lists)
		* [Search Operations](#search-operations)
		* [The Node](#the-node)
		* [Traversing the List](#traversing-the-list)
		* [Inserting into the List](#inserting-into-the-list)
		* [Deleting from the List](#deleting-from-the-list)
	* [Arrays vs Linked Lists](#arrays-vs-linked-lists)
* [L06 - Binary Search Trees](#l06---binary-search-trees)
	* [Types](#types)
	* [Best Case Run Time](#best-case-run-time)
	* [Worst Case Run Time](#worst-case-run-time)
* [L07 - AVL Trees](#l07---avl-trees)
	* [General Procedure](#general-procedure)
	* [Rotation](#rotation)
* [L07/L08 - Deletion in BST](#l07/l08---deletion-in-bst)
	* [Tree Traversal](#tree-traversal)
	* [In-order Traversal](#in-order-traversal)
	* [Deleting an Item](#deleting-an-item)
		* [Node With Two Children](#node-with-two-children)
	* [Deletion Complexity Analysis](#deletion-complexity-analysis)
		* [Worst Case:](#worst-case)
		* [Average Case:](#average-case)
* [L09 - Multi-File Programming](#l09---multi-file-programming)
	* [Header Files](#header-files)
	* [Include](#include)
	* [Makefiles](#makefiles)
# L02 - Algorithms
[← Return to Index](#table-of-contents)


An algorithm is **a set of steps** to accomplish a task.

Computer algorithms must be:

* Specific
* Correct
* Reasonably efficient

## Example: *Fibonacci Numbers*

If we wish to compute the *n*'th number of the Fibonacci sequence, we do so with the following algorithm:

* F(n) = F (n-1) + F (n-2)
* Base Case: **F (1) = F(2) = 1** or **F (0) = 0**, **F(1) = 1**

If T(n) is the ***number of operations*** to calculate the ***n'th*** Fibonacci number, then 

* T(n) = T(n-1) + T(n-2) + 3
* The '3' comes from comparing if n == 0, then n == 1, then the summation of **F(n-1)** and **F(n-2)**

## Memoization

* Store previously computed values
* For example, if whenever we calculate F(n) we store it in an array at index *n* then anytime we need F(n) we can look it up in the array instead of re-calculating it
* This is trade off between space and time, as meoization is faster but it takes up more space

## Complexity Analysis: General Method

**Count operations** for T(n) "time" (number of ops) taken for input n

* It is best to identify the **most expensive operation** (i.e. drop anything that becomes insignificant)
* We can sometimes trade off *space* for *time*



# L03 - Complexity Analysis
[← Return to Index](#table-of-contents)


We want to characterize the run time of **any** algorithm

* Identify the **most expensive operation**
* Count that operation
* Express in terms of input size n

## Big-O Definition

* For two functions *f(n)* and *g(n)*, we say that *f(n)* is *O(g(n))* if there are constants c and N such that f(n) \< c * g(n) for all n > N
* n^2 +33 is in O(n^2) because c = 2 and N= sqrt(33) then n^2+33 < 2n^2
* n^2 + 33n + 17 is in O(n^2)
  * Because c = 2, N = 34 and as such n^2 +33n + 17 < 2n^2 for all n > 34

## Big-O Arithmetic

* If a program is in stages:
  * Stage 1 operates on m inputs in **O(n)**
  * Then Stage 2 operates on n inputs in **O(m)**
  * The whole program is then 
    * **O(m)** + **O(n)** = **O(m+n)**
    * If m << n then just **O(n)**
* If the program operates on each of *n* inputs *m* times the program is **O(m*n)**

## Big-O Hierarchy

It should be memorized that n! >> 2^n >> n^3 >> n^2 >> n log n >> n >> log n >> 1

* Log base does not matter

## Big Omega Ω (Lower Bound) and Big Theta Ө (Growth Rate) 

For two function *f(n)* and *g(n)*, we say that *f(n)* is *Ω(g(n))* if there are constants c and N such that f(n) > c * g(n) for all n > N



# L04 - Data Structures 101
[← Return to Index](#table-of-contents)


A lightning tour of fundamental data structures used for search.

## Abstract Data Types vs. Data Structures

* Abstract Data Types are things like stacks, queue, dictionary
  * Does not specify an implementation
* Concrete data structures
  * Array, linked list, tree
  * Implements an abstract data type

## Data Structures and Searching

* Organising data is important
* It is helpful to organise data **with the task in mind**

## malloc()

* malloc(size_t size)
* size_t is
  * An unsigned integer
  * Can be returned by the *sizeof* operator
  * Widely used in **stdlib** to represent sizes
  * *e.g.* malloc(sizeof(int))

```C
int A[NUMBER];
/**
 * while insertions < NUMBER array is OK
 * BUT… has a limit
 */
int* B;
/* always check return value of malloc()*/
if( (B = (int *) calloc( NUMBER * sizeof(int) )) == NULL )
{
 printf(“malloc() error\n”);
 exit(1);
}
/* B now comes with each slot initialized to 0 */
```



# L05 - Data Structures, *The Basics*
[← Return to Index](#table-of-contents)


## Linked Lists

* Each item (*key*) is located in an arbitrary place in memory
* With a link (pointer) to the next item

### Search Operations

* If unsorted, finding an item in a linked list is still in Ө(n) time.
* Once an insertion point has been determined, it is easy to insert (or delete) a new item, by rearranging links.
* It takes extra space for each item in the list, and extra time to allocate the memory for the node for each item.

### The Node

```c
typedef struct node {
 record r; 
 struct node *next;
 } node_t;

typedef node_t* node_ptr;

node_ptr newnode;
```

###  Traversing the List

```c
p = listhead;
/* Check if list is empty */
if (p != NULL) {
    /* Check if there exists a next node */
    while (p -> next != NULL){
        /* Print node information */
        printf("%d\n", p->key);
        /* Move to next node */
        p = p -> next;
    }
    /* If there does not exist a next node, we reached the last node */
    printf("%d\n", p -> key);
}
```

### Inserting into the List

```C
/* First, you find a correct place in the list to insert the node */

/* Adjust the pointers, for a parent node q that points to p */
q -> next = newnode;
newnode -> next = p

```

### Deleting from the List

```c 
/* We want to delete p */
q -> next = p -> next;
/* Free memory used at address p */
free(p);
```

## Arrays vs Linked Lists

Sorted arrays have a **fast binary search** but **slow insertion** in order to keep it sorted.

Linked lists, we can't do binary search even if it is sorted, as we have to traverse through the whole list.

|                      | One Search | One Insert |
| -------------------- | :--------: | :--------: |
| Unsorted Array       |     n      |     1      |
| Sorted Array         |   log n    |     n      |
| Unsorted Linked List |     n      |     1      |
| Sorted Linked List   |     n      |     n      |

We can see that using a sorted linked list provides no gain.

# L06 - Binary Search Trees
[← Return to Index](#table-of-contents)


*From [Wikipedia](https://en.wikipedia.org/wiki/Binary_search_tree)*

A **binary tree** is a [tree data structure](https://en.wikipedia.org/wiki/Tree_structure) in which each node has at most two [children](https://en.wikipedia.org/wiki/Child_node), which are referred to as the *left child* and the *right child*.

A binary **search** tree is a sorted binary tree where the left child of a node is \<= than the node's value, and the right child of the node is \> the nodes value.

## Types

![Image result for types of binary trees](http://www.csie.ntnu.edu.tw/~u91029/BinaryTree2.png) 



| Type of Tree |                         Description                          |
| :----------: | :----------------------------------------------------------: |
|  Full Tree   |            Every node has either 0 or 2 children             |
|   Complete   | Every level is completely filled (except possibly the last)  |
|   Perfect    | All inner nodes have two children and all leaves have the same depth or same level |

 

## Best Case Run Time

* When it is *perfectly balanced*
* Height of tree with *n* items is at most the ceiling of *log_2(n)*
  * Ceiling means the upper value (ceiling of 2.5 is 3)
  * Quite simple: a perfectly balanced tree with 20 items will have a height of  5
* Insertion/search/deletion are all *O(log n)* for a well-balanced tree 

## Worst Case Run Time 

* The worst case run time occurs when items are inserted in a sorted order. 
* This makes the BST degenerate into a linked list
* The height of this tree of *n* items is *n*
* Insertion/search/deletion are all O(n), yikes!

Evidently, a balanced tree is preferred. 

# L07 - AVL Trees
[← Return to Index](#table-of-contents)


A valiant attempt at getting a [binary search tree](#l06---binary-search-trees) to stay balanced! 

*But why?* Most operations on a BST take time proportional to the height of the tree, so it is desirable to keep the height small. 

**The idea:** Make BST as close to [perfectly balanced](#best-case-run-time) as possible.

There is overhead to this operation. With every insertion into a BST, a *rotation* may be performed to keep it balanced. As such, the Big O time complexity of insertion is:O(n log n).

However, search is guaranteed to be O(log n).

There are many balanced tree implementations, but we care only about **AVL Trees**.

## General Procedure

* Insert node in its appropriate location as you would to a regular BST.

* For each node, we can evaluate the *balance factor* for the node

  ##### Balance Factor

  Balance_Factor(N) = Height(Right_Subtree(N)) – Height(Left_Subtree(N)) 

* If the difference is > 1, then we balance using an operation called **rotation**

Note: A **binary tree** is an **AVL tree** as long as each node has a balance factor that is -1, 0 or 1

## Rotation

[A highly suggested 'click here' for an in-depth look at AVL Tree rotations](avl-tree-rotation.pdf)

The general idea:

* Single rotation if the balance factor signs match
  * For a left heavy tree, do a right rotation
  * For a right heavy tree, do a left rotation
* Double rotation if the balance factor signs do not match
  * For a left heavy tree, do a left rotation on the left subtree, then a right rotation on the root tree
  * For a right heavy tree, do a right rotation on the right subtree, then a left rotation on the root tree 

Rotations are not specific to AVL trees, and appear in other implementations of data structures.

Overall, it is important to remember we are working with [**binary search trees**](#l06---binary-search-trees), which means any rotation must result in a tree which is a valid binary search tree, that is: all node values to the right of a node are greater than the current node value, and vice versa for the nodes to the left.

# L07/L08 - Deletion in BST
[← Return to Index](#table-of-contents)


*Not useful for assignment, but is **tested in exam!***

Deletion from a tree is not as simple as deleting from an array or list. If we delete something, we have to identify what node to substitute it with.

## Tree Traversal

* Visit every node once
* Do something during the visit to the node:
  * Print the node value, or
  * Mark the node as visited, or
  * Check some property of the node

Not specific to trees, as we can also traverse graphs and lists. Thankfully, traversing trees are easier than graphs, as trees are not cyclical (no node points to a parent node).

Example tree:

![Tree](https://i.imgur.com/trNKjd2.png)

**Post-order Traversal**
*Left, Right, Visit*

```C
traverse(t->left);
visit(t);
traverse(t->right);
```

G, F, K, J, Q, S, P, M

You use this to free the nodes of the tree! **You cannot free a tree by just freeing the root**, because the memory addresses to other nodes still exist.

**Pre-order Traversal**
*Visit, Left, Right*

``` 
visit(t);
traverse(t->left);
traverse(t->right);
```

M, J, F, G, K, P, S, Q

## In-order Traversal

*Left, Visit, Right*

F, G, J, K, M, P, Q, S

```C
void traverse(struct node *t)
{
    if(t != NULL)
    {
        traverse(t->left);
        visit(t);
        traverse(t->right);
    }
}
```

## Deleting an Item

* Step 1: Find node to delete
* Step 2: Delete it
  * Case 1: Node is a leaf (no children)
    * Just delete it, ez
  * Case 2: Node has either a left **or** right child, ***not both***
    * Replace with the left or right child respectively
  * Case 3: Node has both a left child **and** a right child!
    * This requires some analysis

### Node With Two Children

Referring back to this example:

![Tree](https://i.imgur.com/trNKjd2.png)

The nodes with two children here are **M** and **J**.

If we want to delete **J**, we have two candidates (inner-most predecessor/successor) which are **K** and **G**. It doesn't matter what one is chosen. We replace **J** with **K** or **G** and still preserve the BST.

If we want to delete **M** our ideal candidates are **K** (inner most predecessor) or **P** (inner-most successor). **K** has no children and as such is a technically better candidate.

**In general, just replace it with the innermost successor (minimum value of right subtree)**

## Deletion Complexity Analysis

### Worst Case: 

* Time to find node is *O(n)*
* Time to find in-order predecessor or successor is *O(n)*
* Total time: *O(2n) = O(n)*

### Average Case:

* Time to find node is *O(log n)*
* Time to find in-order predecessor or successor is *O(log n)* if we specify one, otherwise it is *O(n)*
* Total time is therefore *O(log n)* or *O(n)*

# L09 - Multi-File Programming
[← Return to Index](#table-of-contents)


## Header Files

Just like libraries in C# and Python. 

It is a **.h** file that includes declarations and macro definitions to be shared amongst other source files.

## Include

```C
/* Include file from system path for /include */
#include <file>
/* Include custom file of your own program*/
#include "file.h"
```

The include directive works by telling the C preprocessor to read the specified files before the rest of the current file.

## Makefiles

*[This is a good tutorial, and covers some extra makefile topics too.](http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/)*

* Simplifies compilation of multiple files

Take these three files for example:

**hello.c**

``` C
#include <hellomake.h>
int main() {
  // call a function in another file
  myPrintHelloMake();
  return(0);
}
```

**hellofunc.c** (function definitions)

```c
#include <stdio.h>
#include <hellomake.h>

/* Uses prototype from header file */
void myPrintHelloMake() {
  printf("Hello makefiles!\n");
  return;
}
```

**hellomake.h** (function declarations)

```c
void myPrintHelloMake(void);
```

Normally you would compile this with `gcc -o hellomake hellomake.c hellofunc.c`

> Unfortunately, this approach to compilation has two downfalls. First, if you lose the compile command or switch computers you have to retype it from scratch, which is inefficient at best. Second, if you are only making changes to one .c file, recompiling all of them every time is also time-consuming and inefficient. So, it's time to see what we can do with a makefile. 

The simplest makefile you could create looks like this:

```makefile
hellomake: hellomake.c hellofunc.c
     gcc -o hellomake hellomake.c hellofunc.c
```

The first line let's the compiler know that if **`hellomake.c`** or **`hellofunc.c`** change then to recompile them.

You can see in the second line, there is a tab before the gcc command. There must be a tab before any command or `make` won't be happy
