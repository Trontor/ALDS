# Table of Contents
* [L02 - Algorithms](#l02---algorithms)
	* [Example: Fibonacci Numbers](#example-fibonacci-numbers)
	* [Memoization](#memoization)
	* [Complexity Analysis: General Method](#complexity-analysis-general-method)
* [L03 - Complexity Analysis](#l03---complexity-analysis)
	* [Big-O Definition](#big-o-definition)
	* [Big-O Arithmetic](#big-o-arithmetic)
	* [Big-O Hierarchy](#big-o-hierarchy)
	* [Big Omega Ω (Lower Bound)](#big-omega-Ω-lower-bound)
	* [Big Theta Ө (Growth Rate)](#big-theta-Ө-growth-rate)
* [L04 - Data Structures 101](#l04---data-structures-101)
	* [Abstract Data Types vs. Data Structures](#abstract-data-types-vs.-data-structures)
	* [Data Structures and Searching](#data-structures-and-searching)
	* [malloc()](#malloc)
* [L05 - Data Structures, The Basics](#l05---data-structures,-the-basics)
	* [Linked Lists](#linked-lists)
		* [Search Operations](#search-operations)
		* [The Node](#the-node)
		* [Traversing the List](#traversing-the-list)
		* [Inserting into the List](#inserting-into-the-list)
		* [Deleting from the List](#deleting-from-the-list)
	* [Arrays vs Linked Lists](#arrays-vs-linked-lists)
* [L06 - Binary Search Trees](#l06---binary-search-trees)
	* [Types](#types)
	* [Best Case Run Time](#best-case-run-time)
	* [Worst Case Run Time](#worst-case-run-time)
* [L07 - AVL Trees](#l07---avl-trees)
	* [General Procedure](#general-procedure)
	* [Rotation](#rotation)
* [L07/L08 - Deletion in BST](#l07/l08---deletion-in-bst)
	* [Tree Traversal](#tree-traversal)
	* [In-order Traversal](#in-order-traversal)
	* [Deleting an Item](#deleting-an-item)
		* [Node With Two Children](#node-with-two-children)
	* [Deletion Complexity Analysis](#deletion-complexity-analysis)
		* [Worst Case:](#worst-case)
		* [Average Case:](#average-case)
* [L09 - Multi-File Programming](#l09---multi-file-programming)
	* [Header Files](#header-files)
	* [Include](#include)
	* [Makefiles](#makefiles)
* [L09/10 - Distribution Counting](#l09/10---distribution-counting)
	* [Definition](#definition)
	* [Steps](#steps)
	* [Stable Sort?](#stable-sort?)
	* [Complexity and Example](#complexity-and-example)
		* [Task](#task)
		* [Count](#count)
		* [Cumulative Count](#cumulative-count)
		* [Redistribution](#redistribution)
* [L10/11 - Hash tables](#l10/11---hash-tables)
	* [Complexity](#complexity)
	* [Hash Function](#hash-function)
	* [Modulus and Prime Utilisation](#modulus-and-prime-utilisation)
	* [Hash Function for Strings](#hash-function-for-strings)
	* [Collisions](#collisions)
		* [Chaining](#chaining)
		* [Open Addressing](#open-addressing)
* [L11/12 - Selection/Insertion Sort](#l11/12---selection/insertion-sort)
	* [Selection Sort](#selection-sort)
	* [Insertion Sort](#insertion-sort)
# L02 - Algorithms
[← Return to Index](#table-of-contents)


An algorithm is **a set of steps** to accomplish a task.

Computer algorithms must be:

* Specific
* Correct
* Reasonably efficient

## Example: *Fibonacci Numbers*

If we wish to compute the *n*'th number of the Fibonacci sequence, we do so with the following algorithm:

* F(n) = F (n-1) + F (n-2)
* Base Case: **F (1) = F(2) = 1** or **F (0) = 0**, **F(1) = 1**

If T(n) is the ***number of operations*** to calculate the ***n'th*** Fibonacci number, then 

* T(n) = T(n-1) + T(n-2) + 3
* The '3' comes from comparing if n == 0, then n == 1, then the summation of **F(n-1)** and **F(n-2)**

## Memoization

* Store previously computed values
* For example, if whenever we calculate F(n) we store it in an array at index *n* then anytime we need F(n) we can look it up in the array instead of re-calculating it
* This is trade off between space and time, as meoization is faster but it takes up more space

## Complexity Analysis: General Method

**Count operations** for T(n) "time" (number of ops) taken for input n

* It is best to identify the **most expensive operation** (i.e. drop anything that becomes insignificant)
* We can sometimes trade off *space* for *time*



# L03 - Complexity Analysis
[← Return to Index](#table-of-contents)


We want to characterize the run time of **any** algorithm

* Identify the **most expensive operation**
* Count that operation
* Express in terms of input size n

## Big-O Definition

* For two functions *f(n)* and *g(n)*, we say that *f(n)* is *O(g(n))* if there are constants c and N such that f(n) \< c * g(n) for all n > N
* n^2 +33 is in O(n^2) because c = 2 and N= sqrt(33) then n^2+33 < 2n^2
* n^2 + 33n + 17 is in O(n^2)
  * Because c = 2, N = 34 and as such n^2 +33n + 17 < 2n^2 for all n > 34

## Big-O Arithmetic

* If a program is in stages:
  * Stage 1 operates on m inputs in **O(n)**
  * Then Stage 2 operates on n inputs in **O(m)**
  * The whole program is then 
    * **O(m)** + **O(n)** = **O(m+n)**
    * If m << n then just **O(n)**
* If the program operates on each of *n* inputs *m* times the program is **O(m*n)**

## Big-O Hierarchy

It should be memorized that n! >> 2^n >> n^3 >> n^2 >> n log n >> n >> log n >> 1

* Log base does not matter

## Big Omega Ω (Lower Bound) 

For two function *f(n)* and *g(n)*, we say that *f(n)* is *Ω(g(n))* if there are constants c and N such that f(n) > c * g(n) for all n > N

## Big Theta Ө (Growth Rate) 

For two functions *f(n)* and *g(n)* if there exists three constants c, d, N > 0 such that:

` c * g(n) >= f(n) >= d*g(n) for all n > N `

 then f(n) is in Ө(g(n))

# L04 - Data Structures 101
[← Return to Index](#table-of-contents)


A lightning tour of fundamental data structures used for search.

## Abstract Data Types vs. Data Structures

* Abstract Data Types are things like stacks, queue, dictionary
  * Does not specify an implementation
* Concrete data structures
  * Array, linked list, tree
  * Implements an abstract data type

## Data Structures and Searching

* Organising data is important
* It is helpful to organise data **with the task in mind**

## malloc()

* malloc(size_t size)
* size_t is
  * An unsigned integer
  * Can be returned by the *sizeof* operator
  * Widely used in **stdlib** to represent sizes
  * *e.g.* malloc(sizeof(int))

```C
int A[NUMBER];
/**
 * while insertions < NUMBER array is OK
 * BUT… has a limit
 */
int* B;
/* always check return value of malloc()*/
if( (B = (int *) calloc( NUMBER * sizeof(int) )) == NULL )
{
 printf(“malloc() error\n”);
 exit(1);
}
/* B now comes with each slot initialized to 0 */
```



# L05 - Data Structures, *The Basics*
[← Return to Index](#table-of-contents)


## Linked Lists

* Each item (*key*) is located in an arbitrary place in memory
* With a link (pointer) to the next item

### Search Operations

* If unsorted, finding an item in a linked list is still in Ө(n) time.
* Once an insertion point has been determined, it is easy to insert (or delete) a new item, by rearranging links.
* It takes extra space for each item in the list, and extra time to allocate the memory for the node for each item.

### The Node

```c
typedef struct node {
 record r; 
 struct node *next;
 } node_t;

typedef node_t* node_ptr;

node_ptr newnode;
```

###  Traversing the List

```c
p = listhead;
/* Check if list is empty */
if (p != NULL) {
    /* Check if there exists a next node */
    while (p -> next != NULL){
        /* Print node information */
        printf("%d\n", p->key);
        /* Move to next node */
        p = p -> next;
    }
    /* If there does not exist a next node, we reached the last node */
    printf("%d\n", p -> key);
}
```

### Inserting into the List

```C
/* First, you find a correct place in the list to insert the node */

/* Adjust the pointers, for a parent node q that points to p */
q -> next = newnode;
newnode -> next = p

```

### Deleting from the List

```c 
/* We want to delete p */
q -> next = p -> next;
/* Free memory used at address p */
free(p);
```

## Arrays vs Linked Lists

Sorted arrays have a **fast binary search** but **slow insertion** in order to keep it sorted.

Linked lists, we can't do binary search even if it is sorted, as we have to traverse through the whole list.

|                      | One Search | One Insert |
| -------------------- | :--------: | :--------: |
| Unsorted Array       |     n      |     1      |
| Sorted Array         |   log n    |     n      |
| Unsorted Linked List |     n      |     1      |
| Sorted Linked List   |     n      |     n      |

We can see that using a sorted linked list provides no gain.

# L06 - Binary Search Trees
[← Return to Index](#table-of-contents)


*From [Wikipedia](https://en.wikipedia.org/wiki/Binary_search_tree)*

A **binary tree** is a [tree data structure](https://en.wikipedia.org/wiki/Tree_structure) in which each node has at most two [children](https://en.wikipedia.org/wiki/Child_node), which are referred to as the *left child* and the *right child*.

A binary **search** tree is a sorted binary tree where the left child of a node is \<= than the node's value, and the right child of the node is \> the nodes value.

## Types

![Image result for types of binary trees](http://www.csie.ntnu.edu.tw/~u91029/BinaryTree2.png) 



| Type of Tree |                         Description                          |
| :----------: | :----------------------------------------------------------: |
|  Full Tree   |            Every node has either 0 or 2 children             |
|   Complete   | Every level is completely filled (except possibly the last)  |
|   Perfect    | All inner nodes have two children and all leaves have the same depth or same level |

 

## Best Case Run Time

* When it is *perfectly balanced*
* Height of tree with *n* items is at most the ceiling of *log_2(n)*
  * Ceiling means the upper value (ceiling of 2.5 is 3)
  * Quite simple: a perfectly balanced tree with 20 items will have a height of  5
* Insertion/search/deletion are all *O(log n)* for a well-balanced tree 

## Worst Case Run Time 

* The worst case run time occurs when items are inserted in a sorted order. 
* This makes the BST degenerate into a linked list
* The height of this tree of *n* items is *n*
* Insertion/search/deletion are all O(n), yikes!

Evidently, a balanced tree is preferred. 

# L07 - AVL Trees
[← Return to Index](#table-of-contents)


A valiant attempt at getting a [binary search tree](#l06---binary-search-trees) to stay balanced! 

*But why?* Most operations on a BST take time proportional to the height of the tree, so it is desirable to keep the height small. 

**The idea:** Make BST as close to [perfectly balanced](#best-case-run-time) as possible.

There is overhead to this operation. With every insertion into a BST, a *rotation* may be performed to keep it balanced. As such, the Big O time complexity of insertion is:O(n log n).

However, search is guaranteed to be O(log n).

There are many balanced tree implementations, but we care only about **AVL Trees**.

## General Procedure

* Insert node in its appropriate location as you would to a regular BST.

* For each node, we can evaluate the *balance factor* for the node

  ##### Balance Factor

  Balance_Factor(N) = Height(Right_Subtree(N)) – Height(Left_Subtree(N)) 

* If the difference is > 1, then we balance using an operation called **rotation**

Note: A **binary tree** is an **AVL tree** as long as each node has a balance factor that is -1, 0 or 1

## Rotation

[A highly suggested 'click here' for an in-depth look at AVL Tree rotations](avl-tree-rotation.pdf)

The general idea:

* Single rotation if the balance factor signs match
  * For a left heavy tree, do a right rotation
  * For a right heavy tree, do a left rotation
* Double rotation if the balance factor signs do not match
  * For a left heavy tree, do a left rotation on the left subtree, then a right rotation on the root tree
  * For a right heavy tree, do a right rotation on the right subtree, then a left rotation on the root tree 

Rotations are not specific to AVL trees, and appear in other implementations of data structures.

Overall, it is important to remember we are working with [**binary search trees**](#l06---binary-search-trees), which means any rotation must result in a tree which is a valid binary search tree, that is: all node values to the right of a node are greater than the current node value, and vice versa for the nodes to the left.

# L07/L08 - Deletion in BST
[← Return to Index](#table-of-contents)


*Not useful for assignment, but is **tested in exam!***

Deletion from a tree is not as simple as deleting from an array or list. If we delete something, we have to identify what node to substitute it with.

## Tree Traversal

* Visit every node once
* Do something during the visit to the node:
  * Print the node value, or
  * Mark the node as visited, or
  * Check some property of the node

Not specific to trees, as we can also traverse graphs and lists. Thankfully, traversing trees are easier than graphs, as trees are not cyclical (no node points to a parent node).

Example tree:

![Tree](https://i.imgur.com/trNKjd2.png)

**Post-order Traversal**
*Left, Right, Visit*

```C
traverse(t->left);
visit(t);
traverse(t->right);
```

G, F, K, J, Q, S, P, M

You use this to free the nodes of the tree! **You cannot free a tree by just freeing the root**, because the memory addresses to other nodes still exist.

**Pre-order Traversal**
*Visit, Left, Right*

``` 
visit(t);
traverse(t->left);
traverse(t->right);
```

M, J, F, G, K, P, S, Q

## In-order Traversal

*Left, Visit, Right*

F, G, J, K, M, P, Q, S

```C
void traverse(struct node *t)
{
    if(t != NULL)
    {
        traverse(t->left);
        visit(t);
        traverse(t->right);
    }
}
```

## Deleting an Item

* Step 1: Find node to delete
* Step 2: Delete it
  * Case 1: Node is a leaf (no children)
    * Just delete it, ez
  * Case 2: Node has either a left **or** right child, ***not both***
    * Replace with the left or right child respectively
  * Case 3: Node has both a left child **and** a right child!
    * This requires some analysis

### Node With Two Children

Referring back to this example:

![Tree](https://i.imgur.com/trNKjd2.png)

The nodes with two children here are **M** and **J**.

If we want to delete **J**, we have two candidates (inner-most predecessor/successor) which are **K** and **G**. It doesn't matter what one is chosen. We replace **J** with **K** or **G** and still preserve the BST.

If we want to delete **M** our ideal candidates are **K** (inner most predecessor) or **P** (inner-most successor). **K** has no children and as such is a technically better candidate.

**In general, just replace it with the innermost successor (minimum value of right subtree)**

## Deletion Complexity Analysis

### Worst Case: 

* Time to find node is *O(n)*
* Time to find in-order predecessor or successor is *O(n)*
* Total time: *O(2n) = O(n)*

### Average Case:

* Time to find node is *O(log n)*
* Time to find in-order predecessor or successor is *O(log n)* if we specify one, otherwise it is *O(n)*
* Total time is therefore *O(log n)* or *O(n)*

# L09 - Multi-File Programming
[← Return to Index](#table-of-contents)


## Header Files

Just like libraries in C# and Python. 

It is a **.h** file that includes declarations and macro definitions to be shared amongst other source files.

## Include

```C
/* Include file from system path for /include */
#include <file>
/* Include custom file of your own program*/
#include "file.h"
```

The include directive works by telling the C preprocessor to read the specified files before the rest of the current file.

## Makefiles

*[This is a good tutorial, and covers some extra makefile topics too.](http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/)*

* Simplifies compilation of multiple files

Take these three files for example:

**hello.c**

``` C
#include <hellomake.h>
int main() {
  // call a function in another file
  myPrintHelloMake();
  return(0);
}
```

**hellofunc.c** (function definitions)

```c
#include <stdio.h>
#include <hellomake.h>

/* Uses prototype from header file */
void myPrintHelloMake() {
  printf("Hello makefiles!\n");
  return;
}
```

**hellomake.h** (function declarations)

```c
void myPrintHelloMake(void);
```

Normally you would compile this with `gcc -o hellomake hellomake.c hellofunc.c`

> Unfortunately, this approach to compilation has two downfalls. First, if you lose the compile command or switch computers you have to retype it from scratch, which is inefficient at best. Second, if you are only making changes to one .c file, recompiling all of them every time is also time-consuming and inefficient. So, it's time to see what we can do with a makefile. 

The simplest makefile you could create looks like this:

```makefile
hellomake: hellomake.c hellofunc.c
     gcc -o hellomake hellomake.c hellofunc.c
```

The first line let's the compiler know that if **`hellomake.c`** or **`hellofunc.c`** change then to recompile them.

You can see in the second line, there is a tab before the gcc command. There must be a tab before any command or `make` won't be happy

# L09/10 - Distribution Counting
[← Return to Index](#table-of-contents)


## Definition

A sorting algorithm that stores, for each `sortkey`, the number of records with the given `sortkey` (thus anticipating that keys might not be unique). With this information it is possible to place the records correctly into a sorted file. The algorithm is useful when the keys fall into a small range and many of them are equal.

## Steps

- **Input**: *an array of:*

  - Records

    or

  - Keys + Pointers to Records

- **Count** number of records associated with each key value (lower to upper)

- **Redistribute** array elements

- Output: Stable Sorted Array

## Stable Sort?

If we have elements ***R*** and ***S*** (in that order) with the same key and apply a sorting algorithm to it, it is considered a stable sort if ***R*** appears before ***S*** in the sorted array. Because they have the same key, it is easy to switch them up. Stable sorting algorithms preserve the order of elements with the same key.

## Complexity and Example

### Task

Sort `[ 4, 4, 2, 2, 0, 2, 1, 3, 2, 4, 3, 1, 4, 3, 1, 4]` 

### Count

Count the number of records associated with each key value (lower to upper)

Key **0** occurs **1** time

0->1

1->3

2->4

3->3

4->5

To perform this, we traverse this in **O(n)** worst case as a more effective way to do this for a computer is to go through go through once and increment the `counting` array at index `n` as we progress through the input array.

### Cumulative Count

The cumulative count is an array that stores at index `i` the number of elements from the input that is less than `i`

`cumulativeCount = [0, 1, 4, 8, 11]` (0 elements less than 0, 1 element less than 1, etc.)

### Redistribution

We traverse the original input array, and copy each item to its new position as follows

```C 
aux_array[cumulativeCount[item.key]] = item
cumulativeCount[item.key] += 1
```

# L10/11 - Hash tables
[← Return to Index](#table-of-contents)


This is a data structure that provides a means of accessing a desired item directly. For a hash table:

## Complexity

- Search is **O(1)** average case
  - If the hash table is managed well

However, the worst case is **O(n)**. 

## Hash Function

The hash table is an array/dictionary who's index is the result of a hash function. 

![](images/hash.png)

## Modulus and Prime Utilisation

The number of keys that can be output by the hash table is often not known but it can be squished into an array using the modulo operator. You should always use a **prime number** to multiply and as the right hand side term of the modulo command.

`hash(key) = (key * BIGPRIME ) % prime`

## Hash Function for Strings

Use a base mapping **power of 2** 	like so:

If we want to map **c a t** we map it like so:

` H("cat") = 32^2 * 3 + 32 * 1 + 32^0 + 20`

Because **c = 3**, **a = 1**, and **t = 20**

## Collisions

When two keys map to the same array index. Good hash functions have fewer collisions, but we can never assume there will be none.

### Chaining

Add to the end of the linked list at the array entry.

```C
void insert( HT, item )
{
     new newnode = /* ... make a list node */
    /* put --item-- in the list node */
     index = hash(item->key);
    if( HT[ index ] == NULL)
     	HT[ index ] = newnode;
    else
    {
         newnode->next = HT[ index ]->node;
         HT[ index ] = newnode;
    }
}
```

### Open Addressing

##### Linear Probing

If there is a collision, put the item in the next available slot. Catastrophic failure when table full, also clustering can occur easily.

##### Double Hashing

Choose a second hash function.

##### Load Factor **α**

For **n** keys in **m** cells

- α = n/m

Average case, under some simplifying assumptions, expected time for insertion is:

- Double hashing: **1/(1-α)** 
- Linear probing: **1/(1-α)^2**

Example: **α = 0.75**

- Double hash insertion: 4 probes
- Linear probing insertion: 16 probes

Hash tables show fast lookup

# L11/12 - Selection/Insertion Sort
[← Return to Index](#table-of-contents)


Sorting has many applications and is used widely. The two most naïve sorting algorithms are **selection sort** and **insertion sort**.

## Selection Sort

Select the lowest element and swap it with the first unsorted element

- In place
- Worst case **O(n^2)**
- Best case **O(n^2)**
- Average case **O(n^2)**

```C
void selection(item* A, int n)
{
    int i,j,min;
    for (i = 0; i < n-1; i++) { /* why n-1? */ 
        min = i;
        for( j = i+1; j < n; j++ ) {
            if (cmp( A[j], A[min] ) < 0) 
                min = j;
        }
        SWAP( A[i], A[min] );
    }
}
```

## Insertion Sort

Iterate through the list and move each element to the appropriate spot in the array.



For example:

![](images/insertionsort.gif)

```C
void insertionsort()
{ 
    int i, j;
 	for (i = 1; i < n; i++)
        for (j = i; j > 0 && x[j-1] > x[j]; j--)
            swap(j-1, j);
}
```

